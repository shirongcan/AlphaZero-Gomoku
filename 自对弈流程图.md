# AlphaZero 自对弈流程图

```mermaid
flowchart TD
    Start([训练开始]) --> Init[初始化模型和缓冲区]
    Init --> InitModel{是否有预训练模型?}
    InitModel -->|是| LoadModel[加载预训练模型]
    InitModel -->|否| NewModel[创建新模型]
    LoadModel --> InitBuffer
    NewModel --> InitBuffer[初始化经验回放缓冲区]
    InitBuffer --> LoadBuffer{是否存在保存的缓冲区?}
    LoadBuffer -->|是| LoadData[加载历史训练数据]
    LoadBuffer -->|否| EmptyBuffer[空缓冲区开始]
    LoadData --> IterLoop
    EmptyBuffer --> IterLoop{迭代循环<br/>iter = 1 to num_iterations}
    
    IterLoop --> SelfPlay[自对弈生成阶段]
    
    SelfPlay --> SaveModel[保存模型到临时文件]
    SaveModel --> ParWorker[创建多进程池<br/>n_workers 个工作进程]
    ParWorker --> GameLoop{并行生成<br/>n_games 局游戏}
    
    GameLoop --> GameStart[单局游戏开始]
    GameStart --> InitGameState[初始化游戏状态<br/>创建 MCTS 和游戏实例]
    
    InitGameState --> GameStep{游戏循环}
    GameStep --> EncodeState[获取当前状态编码<br/>state_enc]
    EncodeState --> MCTS[MCTS 搜索<br/>n_simulations 次模拟]
    MCTS --> GetPolicy[获得策略向量 π]
    GetPolicy --> CalcTemp[计算温度<br/>temp = f(move_number)]
    CalcTemp --> SampleAction[根据温度和 π 采样动作<br/>temp>0: 随机采样<br/>temp=0: argmax]
    SampleAction --> Validate[验证动作合法性]
    Validate -->|不合法| ArgmaxAction[使用 argmax 动作]
    Validate -->|合法| StoreData
    ArgmaxAction --> StoreData[存储训练样本<br/>(state_enc, π, current_player)]
    StoreData --> ExecuteMove[执行动作<br/>game.do_move]
    ExecuteMove --> CheckGameOver{游戏是否结束?}
    CheckGameOver -->|否| MoveInc[移动数 +1]
    MoveInc --> CheckMaxMove{达到最大步数?}
    CheckMaxMove -->|否| GameStep
    CheckMaxMove -->|是| EndGame
    CheckGameOver -->|是| EndGame[游戏结束]
    
    EndGame --> GetWinner[获取获胜者<br/>winner = 0/1/2]
    GetWinner --> ProcessReward[处理奖励<br/>z = 1.0 if player wins<br/>z = -1.0 if player loses<br/>z = 0.0 if draw]
    ProcessReward --> ApplySym[应用对称性增强<br/>生成旋转/翻转变体]
    ApplySym --> CollectExamples[收集最终训练样本<br/>(state_aug, π_aug, z)]
    CollectExamples --> CleanMCTS[清理 MCTS 树<br/>释放内存]
    CleanMCTS --> GameResult{所有游戏完成?}
    GameResult -->|否| GameStart
    GameResult -->|是| MergeResults[合并所有游戏数据]
    
    MergeResults --> AddBuffer[添加到经验回放缓冲区]
    AddBuffer --> SaveBuffer[保存缓冲区到磁盘]
    SaveBuffer --> CheckBuffer{缓冲区大小<br/>>= batch_size?}
    
    CheckBuffer -->|是| TrainPhase[训练阶段]
    CheckBuffer -->|否| SkipTrain[跳过训练<br/>样本不足]
    
    TrainPhase --> EpochLoop{训练轮次<br/>epoch = 1 to epochs_per_iter}
    EpochLoop --> SampleBatch[从缓冲区采样<br/>batch_size 个样本]
    SampleBatch --> TrainBatch[训练模型一个批次<br/>更新网络参数]
    TrainBatch --> EpochCheck{完成所有批次?}
    EpochCheck -->|否| SampleBatch
    EpochCheck -->|是| EpochDone{完成所有轮次?}
    EpochDone -->|否| EpochLoop
    EpochDone -->|是| EvalPhase
    
    SkipTrain --> EvalPhase[评估阶段]
    EvalPhase --> EvalGames{并行评估<br/>新模型 vs 最佳模型<br/>n_games 局]
    EvalGames --> EvalGame[单局评估游戏]
    EvalGame --> EvalMCTS1[新模型 MCTS 搜索]
    EvalMCTS1 --> EvalMCTS2[最佳模型 MCTS 搜索]
    EvalMCTS2 --> EvalStep{游戏循环}
    EvalStep -->|新模型回合| EvalMCTS1
    EvalStep -->|最佳模型回合| EvalMCTS2
    EvalStep -->|游戏结束| EvalResult[统计新模型胜局数]
    EvalResult --> EvalDone{所有评估完成?}
    EvalDone -->|否| EvalGame
    EvalDone -->|是| CalcWinRate[计算胜率<br/>win_rate = new_wins / n_games]
    
    CalcWinRate --> Decision{胜率 >= 阈值?}
    Decision -->|是| Accept[接受候选模型<br/>提升为最佳模型]
    Decision -->|否| Reject[拒绝候选模型<br/>从最佳模型恢复]
    
    Accept --> UpdateBest[更新 model_best<br/>保存最佳模型]
    UpdateBest --> CreateCandidate[从最佳模型<br/>创建新候选模型]
    
    Reject --> ResetCandidate[从最佳模型<br/>重置候选模型]
    
    CreateCandidate --> SaveSnapshot{是否需要<br/>保存快照?}
    ResetCandidate --> SaveSnapshot
    SaveSnapshot -->|是| SaveModelFile[保存模型快照<br/>snapshot_iter*.pt]
    SaveSnapshot -->|否| IterDone
    SaveModelFile --> IterDone[迭代完成]
    
    IterDone --> IterCheck{是否还有<br/>更多迭代?}
    IterCheck -->|是| IterLoop
    IterCheck -->|否| Finish([训练完成])
    
    style Start fill:#e1f5e1
    style Finish fill:#e1f5e1
    style SelfPlay fill:#fff4e1
    style TrainPhase fill:#e1f0ff
    style EvalPhase fill:#ffe1f0
    style Accept fill:#d4edda
    style Reject fill:#f8d7da
    style MCTS fill:#f0e1ff
    style GameStep fill:#fff9e1
    style EvalStep fill:#fff9e1
```

## 流程说明

### 1. 初始化阶段
- 检查是否存在预训练模型，如果有则加载
- 初始化经验回放缓冲区，尝试加载历史训练数据

### 2. 自对弈生成阶段（每次迭代）
- 使用多进程并行生成多局自对弈游戏
- 每局游戏：
  - 使用当前候选模型进行 MCTS 搜索
  - 根据温度采样动作（前期探索，后期确定性）
  - 记录每一步的 (状态, 策略, 玩家)
  - 游戏结束后计算奖励并应用对称性增强

### 3. 训练阶段
- 从经验回放缓冲区采样批次数据
- 训练候选模型多个轮次（epochs）
- 更新网络参数以最小化损失函数

### 4. 评估阶段
- 新训练的候选模型 vs 当前最佳模型
- 进行多局评估游戏（轮流先手）
- 统计候选模型胜率

### 5. 接受/拒绝决策
- 如果胜率 >= 阈值：接受候选模型，提升为最佳模型
- 否则：拒绝候选模型，从最佳模型恢复

### 6. 模型保存
- 定期保存模型快照
- 保存最佳模型检查点
- 保存训练数据缓冲区

