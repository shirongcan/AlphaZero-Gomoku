import os
import time
import random
from collections import deque
from typing import List, Tuple, Optional
import numpy as np
from network import PyTorchModel
from mcts.new_mcts_alpha import MCTS
from games.gomoku import Gomoku as GameClass
from datetime import datetime
from copy import deepcopy
import gc
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle


# -------------------------
#  å¤šè¿›ç¨‹è‡ªå¯¹å¼ˆ worker
# -------------------------
_SELFPLAY_MODEL = None
_SELFPLAY_MODEL_META = None  # (model_path, board_size, action_size, device)

# è¯„ä¼° worker æ¨¡å‹ç¼“å­˜ï¼ˆæ¯ä¸ªå­è¿›ç¨‹åªåŠ è½½ä¸€æ¬¡ï¼‰
_EVAL_MODEL_NEW = None
_EVAL_MODEL_BEST = None
_EVAL_MODEL_META = None  # (new_path, best_path, board_size, action_size, device)


def _selfplay_worker_init(
    model_path: str,
    board_size: int,
    action_size: int,
    device: str,
    base_seed: int = 0,
    torch_num_threads: int = 1,
):
    """
    Windows ä¸‹ä½¿ç”¨ spawnï¼šæ¯ä¸ªå­è¿›ç¨‹ä¼šé‡æ–° import æœ¬æ–‡ä»¶ã€‚
    initializer ç”¨äºè®¾å®šéšæœºç§å­ä¸ PyTorch çº¿ç¨‹æ•°ï¼Œé¿å… CPU è¿‡åº¦æŠ¢å ã€‚
    """
    try:
        import torch
        torch.set_num_threads(max(1, int(torch_num_threads)))
    except Exception:
        pass

    pid = os.getpid()
    seed = int(base_seed) + pid
    random.seed(seed)
    np.random.seed(seed % (2**32 - 1))

    # æ¯ä¸ªå­è¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹ï¼ˆé¿å…æ¯ä¸ªä»»åŠ¡é‡å¤ loadï¼‰
    global _SELFPLAY_MODEL, _SELFPLAY_MODEL_META
    _SELFPLAY_MODEL_META = (str(model_path), int(board_size), int(action_size), str(device))

    from network import PyTorchModel  # å»¶è¿Ÿ import
    _SELFPLAY_MODEL = PyTorchModel(board_size=int(board_size), action_size=int(action_size), device=str(device))
    _SELFPLAY_MODEL.load(str(model_path), map_location=str(device))


def _selfplay_generate_games(
    *,
    board_size: int,
    n_simulations: int,
    cpuct: float,
    temp_threshold: int,
    add_dirichlet_noise: bool,
    games_to_play: int,
    use_symmetries: bool,
    max_moves: int,
    dirichlet_alpha: float,
    dirichlet_epsilon: float,
    dirichlet_n_moves: int,
    device: str = "cpu",
) -> Tuple[List[Tuple[np.ndarray, np.ndarray, float]], dict]:
    """
    å­è¿›ç¨‹å…¥å£ï¼šåŠ è½½æ¨¡å‹ -> è·‘è‹¥å¹²å±€è‡ªå¯¹å¼ˆ -> è¿”å›æ ·æœ¬ä¸èƒœè´Ÿç»Ÿè®¡ã€‚
    æ³¨æ„ï¼šä¸ºäº†é¿å… GPU å¤šè¿›ç¨‹äº‰ç”¨ï¼Œé»˜è®¤ device=cpuã€‚
    """
    from mcts.new_mcts_alpha import MCTS
    from games.gomoku import Gomoku as GameClass

    # å¤šè¿›ç¨‹è·¯å¾„ï¼šä¼˜å…ˆå¤ç”¨ initializer åŠ è½½çš„å…¨å±€æ¨¡å‹
    global _SELFPLAY_MODEL, _SELFPLAY_MODEL_META
    model = _SELFPLAY_MODEL
    if model is None:
        # å…¼å®¹ï¼šå¦‚æœæ²¡æœ‰èµ° initializerï¼ˆä¾‹å¦‚å•è¿›ç¨‹/ç›´æ¥è°ƒç”¨ï¼‰ï¼Œå†æœ¬åœ°åˆ›å»ºæ¨¡å‹
        from network import PyTorchModel
        model = PyTorchModel(board_size=board_size, action_size=board_size * board_size, device=device)

    def temp_fn(move_number: int):
        return max(0.0, 1.0 - move_number / temp_threshold)

    winners = {0: 0, 1: 0, 2: 0}
    all_examples: List[Tuple[np.ndarray, np.ndarray, float]] = []

    for _ in range(int(games_to_play)):
        mcts_play = MCTS(
            game_class=GameClass,
            n_simulations=n_simulations,
            nn_model=model,
            cpuct=cpuct,
            dirichlet_alpha=dirichlet_alpha,
            epsilon=dirichlet_epsilon,
            apply_dirichlet_n_first_moves=dirichlet_n_moves,
            add_dirichlet_noise=add_dirichlet_noise,
        )
        game = GameClass(size=board_size)
        game.current_player = 1

        examples, winner = play_game_and_collect(
            mcts_play,
            game,
            temp_fn,
            max_moves=max_moves,
            use_symmetries=use_symmetries,
        )
        all_examples.extend(examples)
        winners[winner] = winners.get(winner, 0) + 1

        mcts_play.clear_tree()
        del mcts_play

    # æ˜¾å¼é‡Šæ”¾
    # å¦‚æœæ˜¯å…¨å±€å¤ç”¨æ¨¡å‹ï¼Œä¸åœ¨è¿™é‡Œé‡Šæ”¾ï¼›è®©å­è¿›ç¨‹é€€å‡ºæ—¶ç»Ÿä¸€å›æ”¶
    gc.collect()

    return all_examples, winners

# -------------------------
#  å¤šè¿›ç¨‹è¯„ä¼° worker
# -------------------------
def _eval_worker_init(
    model_new_path: str,
    model_best_path: str,
    board_size: int,
    action_size: int,
    device: str,
    base_seed: int = 0,
    torch_num_threads: int = 1,
):
    try:
        import torch
        torch.set_num_threads(max(1, int(torch_num_threads)))
    except Exception:
        pass

    pid = os.getpid()
    seed = int(base_seed) + pid
    random.seed(seed)
    np.random.seed(seed % (2**32 - 1))

    global _EVAL_MODEL_NEW, _EVAL_MODEL_BEST, _EVAL_MODEL_META
    _EVAL_MODEL_META = (str(model_new_path), str(model_best_path), int(board_size), int(action_size), str(device))

    from network import PyTorchModel  # å»¶è¿Ÿ import
    _EVAL_MODEL_NEW = PyTorchModel(board_size=int(board_size), action_size=int(action_size), device=str(device))
    _EVAL_MODEL_NEW.load(str(model_new_path), map_location=str(device))

    _EVAL_MODEL_BEST = PyTorchModel(board_size=int(board_size), action_size=int(action_size), device=str(device))
    _EVAL_MODEL_BEST.load(str(model_best_path), map_location=str(device))


def _eval_play_games(
    *,
    board_size: int,
    n_games: int,
    start_index: int,
    n_simulations: int,
    cpuct: float,
) -> Tuple[int, int, int]:
    """
    å­è¿›ç¨‹è¯„ä¼°ï¼šè·‘ n_games å±€ï¼Œä½¿ç”¨ start_index æ¥å†³å®šäº¤æ›¿å…ˆæ‰‹ã€‚
    è¿”å› (new_wins, draws, total_games)
    """
    from mcts.new_mcts_alpha import MCTS
    from games.gomoku import Gomoku as GameClass

    global _EVAL_MODEL_NEW, _EVAL_MODEL_BEST
    model_new = _EVAL_MODEL_NEW
    model_best = _EVAL_MODEL_BEST

    new_wins = 0
    draws = 0

    for gi in range(int(n_games)):
        global_i = int(start_index) + gi

        game = GameClass(size=int(board_size))
        # éšæœºç¬¬ä¸€æ‰‹ï¼Œå¢åŠ å¼€å±€å¤šæ ·æ€§ï¼ˆè¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§é€‰æ‹©ï¼Œä¸éšæœºä¼šå¯¼è‡´æ‰€æœ‰å¯¹å±€ç›¸åŒï¼‰
        # é™åˆ¶åœ¨ä¸­å¿ƒ9Ã—9åŒºåŸŸï¼Œé¿å…è¾¹è§’çš„ä¸åˆç†å¼€å±€
        # ç¬¬ä¸€æ‰‹ï¼ˆç©å®¶1ï¼‰
        center = int(board_size) // 2
        radius = 4  # 9Ã—9åŒºåŸŸ (81ç§å¯èƒ½ Ã— 2å…ˆåæ‰‹ = 162ç§ç»„åˆ)
        r1 = random.randint(center - radius, center + radius)
        c1 = random.randint(center - radius, center + radius)
        game.do_move((r1, c1))
        # ç°åœ¨ current_player = 2ï¼Œä»ç¬¬äºŒæ‰‹å¼€å§‹çœŸæ­£è¯„ä¼°

        new_starts = (global_i % 2 == 0)
        move_number = 1

        mcts_new = MCTS(
            game_class=GameClass,
            n_simulations=n_simulations,
            nn_model=model_new,
            cpuct=cpuct,
            add_dirichlet_noise=False,
        )
        mcts_best = MCTS(
            game_class=GameClass,
            n_simulations=n_simulations,
            nn_model=model_best,
            cpuct=cpuct,
            add_dirichlet_noise=False,
        )

        while not game.is_game_over():
            if (game.current_player == 1 and new_starts) or (game.current_player == 2 and not new_starts):
                pi = mcts_new.run(game, len(game.move_history))
            else:
                pi = mcts_best.run(game, len(game.move_history))

            action = int(np.argmax(pi))
            rr, cc = divmod(action, game.size)
            game.do_move((rr, cc))
            move_number += 1
            if move_number > game.size * game.size:
                break

        winner = game.get_winner()
        if winner == 0:
            draws += 1
        else:
            if (winner == 1 and new_starts) or (winner == 2 and not new_starts):
                new_wins += 1

        mcts_new.clear_tree()
        mcts_best.clear_tree()
        del mcts_new
        del mcts_best

    gc.collect()
    return int(new_wins), int(draws), int(n_games)

# åœ¨å¾ªç¯å†…åŠ¨æ€æ˜ å°„æ¸¸æˆåç§°åˆ°ç±»

# -------------------------
#  å·¥å…·å‡½æ•°
# -------------------------
def softmax_temperature(pi: np.ndarray, temp: float) -> np.ndarray:
    if temp <= 0:
        return pi
    logits = np.log(pi + 1e-15)
    logits = logits / temp
    exps = np.exp(logits - np.max(logits))
    p = exps / np.sum(exps)
    return p


def sample_action_from_pi(pi: np.ndarray, temp: float) -> int:
    if temp == 0:
        return int(np.argmax(pi))
    p = softmax_temperature(pi, temp)
    return int(np.random.choice(len(p), p=p))


# -------------------------
#  ç»éªŒå›æ”¾ç¼“å†²åŒº
# -------------------------
class ReplayBuffer:
    def __init__(self, capacity: int = 20000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)

    def add(self, examples: List[Tuple[np.ndarray, np.ndarray, float]]):
        """
        æ·»åŠ ç¤ºä¾‹åˆ—è¡¨ (state_enc, pi, z)
        state_enc: (C,H,W)
        pi: (action_size,)
        z: æ ‡é‡ (-1,0,1)
        """
        for ex in examples:
            self.buffer.append(ex)

    def sample(self, batch_size: int):
        batch = random.sample(self.buffer, k=batch_size)
        states, pis, zs = zip(*batch)
        states = np.stack(states, axis=0).astype(np.float32)
        pis = np.stack(pis, axis=0).astype(np.float32)
        zs = np.array(zs, dtype=np.float32).reshape(-1, 1)
        return states, pis, zs

    def __len__(self):
        return len(self.buffer)


# -------------------------
#  Buffer æŒä¹…åŒ–
# -------------------------
def save_replay_buffer(buffer: ReplayBuffer, filepath: str):
    """
    ä¿å­˜ ReplayBuffer åˆ°ç£ç›˜
    åªä¿å­˜ buffer å†…å®¹ï¼Œä¸ä¿å­˜ capacityï¼ˆåœ¨åŠ è½½æ—¶é‡æ–°è®¾å®šï¼‰
    """
    try:
        # å°† deque è½¬ä¸º list ä»¥ä¾¿ pickle
        buffer_data = {
            'buffer': list(buffer.buffer),
            'capacity': buffer.capacity
        }
        with open(filepath, 'wb') as f:
            pickle.dump(buffer_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"[Buffer] å·²ä¿å­˜åˆ°: {filepath} (å¤§å°: {len(buffer)} æ ·æœ¬)")
        return True
    except Exception as e:
        print(f"[Buffer] ä¿å­˜å¤±è´¥: {e}")
        return False


def load_replay_buffer(filepath: str, capacity: int) -> Optional[ReplayBuffer]:
    """
    ä»ç£ç›˜åŠ è½½ ReplayBuffer
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œè¿”å› None
    """
    if not os.path.exists(filepath):
        print(f"[Buffer] æœªæ‰¾åˆ°å·²ä¿å­˜çš„ buffer: {filepath}")
        return None
    
    try:
        with open(filepath, 'rb') as f:
            buffer_data = pickle.load(f)
        
        # åˆ›å»ºæ–°çš„ ReplayBuffer
        buffer = ReplayBuffer(capacity=capacity)
        
        # æ¢å¤æ•°æ®
        saved_buffer = buffer_data['buffer']
        saved_capacity = buffer_data.get('capacity', capacity)
        
        # å¦‚æœä¿å­˜çš„å®¹é‡ä¸å½“å‰é…ç½®ä¸åŒï¼Œç»™å‡ºè­¦å‘Š
        if saved_capacity != capacity:
            print(f"[Buffer] è­¦å‘Š: ä¿å­˜çš„å®¹é‡ ({saved_capacity}) ä¸å½“å‰é…ç½® ({capacity}) ä¸åŒ")
        
        # å°†æ•°æ®æ·»åŠ å› bufferï¼ˆdeque ä¼šè‡ªåŠ¨å¤„ç† maxlenï¼‰
        for item in saved_buffer:
            buffer.buffer.append(item)
        
        print(f"[Buffer] å·²åŠ è½½: {filepath} (å¤§å°: {len(buffer)} æ ·æœ¬)")
        return buffer
    except Exception as e:
        print(f"[Buffer] åŠ è½½å¤±è´¥: {e}")
        return None


# -------------------------
#  è‡ªå¯¹å¼ˆå•å±€æ¸¸æˆ
# -------------------------
def play_game_and_collect(mcts: MCTS, game, temp_fn, max_moves=225, use_symmetries=True):
    """
    è¿›è¡Œä¸€å±€å®Œæ•´çš„æ¸¸æˆå¹¶è¿”å›å¢å¼ºåçš„ç¤ºä¾‹ï¼š
    final_examples: (state_enc (C,H,W), pi (A,), z æ ‡é‡) çš„åˆ—è¡¨
    winner: 0/1/2
    """
    examples = []
    move_number = 0

    while True:
        state_enc = game.get_encoded_state()  # æœŸæœ›æ˜¯è§†è§’ä¸å˜çš„
        pi = mcts.run(game, len(game.move_history))  # å‘é‡ (action_size,) ç¬¬äºŒä¸ªå‚æ•°æ˜¯å½“å‰æ˜¯ç¬¬å‡ æ­¥
        # è¿™ä¸ªå‚æ•°æ˜¯è®©MCtS çŸ¥é“å½“å‰æ˜¯ç¬¬å‡ æ­¥,æ˜¯ä¸æ˜¯è¦åŠ å…¥dirichlet noiseï¼Œç”¨æ¥å¢å¼ºMCTSçš„æ¢ç´¢èƒ½åŠ›
        # è¿”å›ä¸€ä¸ªå‘é‡ (action_size,) æ¯ä¸ªå…ƒç´ æ˜¯æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
        pi_for_store = pi.copy()

        temp = temp_fn(move_number)
        action = sample_action_from_pi(pi, temp)

        # å®‰å…¨å›é€€ï¼šå¦‚æœé€‰æ‹©çš„åŠ¨ä½œä¸åˆæ³•ï¼Œä½¿ç”¨ argmax
        valid_mask = game.get_valid_moves()
        if valid_mask[action] != 1.0:
            action = int(np.argmax(pi))
        # å­˜å‚¨ (state, pi, player)
        examples.append((state_enc, pi_for_store, int(game.current_player)))

        # æ‰§è¡Œç§»åŠ¨
        r, c = divmod(action, game.size)
        game.do_move((r, c))

        move_number += 1

        if game.is_game_over() or move_number >= max_moves:
            break

    winner = game.get_winner()  # 0/1/2

    # å°†ç¤ºä¾‹è½¬æ¢ä¸º (state_aug, pi_aug, z)
    final_examples = []
    for state_enc, pi_vec, player in examples:
        if winner == 0:
            z = 0.0
        else:
            z = 1.0 if winner == player else -1.0

        if use_symmetries:
            syms = mcts.symmetries(state_enc, pi_vec)
            for s_aug, pi_aug in syms:
                final_examples.append((s_aug.astype(np.float32), pi_aug.astype(np.float32), z))
        else:
            final_examples.append((state_enc.astype(np.float32), pi_vec.astype(np.float32), z))

    return final_examples, winner


# -------------------------
#  æ¨¡å‹é—´è¯„ä¼°
# -------------------------
def evaluate_models(model_new: PyTorchModel,
                model_best: PyTorchModel,
                game_name: str,
                n_games: int = 20,
                n_simulations: int = 100,
                cpuct: float = 1.0) -> Tuple[int, float, int]:
    """
    åœ¨ model_new å’Œ model_best ä¹‹é—´è¿›è¡Œ n_games å±€æ¸¸æˆï¼ˆè½®æµå…ˆæ‰‹ï¼‰ã€‚
    è¿”å› (win_rate_of_new, draws)
    """
    # ä»…æ”¯æŒ Gomokuï¼ˆä¿ç•™ game_name å‚æ•°ç”¨äºå‘åå…¼å®¹ï¼‰
    rules_name = "gomoku"

    new_wins = 0
    draws = 0
    total = n_games

    for i in range(n_games):
        game = GameClass(size=model_new.board_size)

        # éšæœºç¬¬ä¸€æ‰‹ï¼Œå¢åŠ å¼€å±€å¤šæ ·æ€§ï¼ˆè¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§é€‰æ‹©ï¼Œä¸éšæœºä¼šå¯¼è‡´æ‰€æœ‰å¯¹å±€ç›¸åŒï¼‰
        # é™åˆ¶åœ¨ä¸­å¿ƒ9Ã—9åŒºåŸŸï¼Œé¿å…è¾¹è§’çš„ä¸åˆç†å¼€å±€
        # ç¬¬ä¸€æ‰‹ï¼ˆç©å®¶1ï¼‰
        center = model_new.board_size // 2
        radius = 4  # 9Ã—9åŒºåŸŸ (81ç§å¯èƒ½ Ã— 2å…ˆåæ‰‹ = 162ç§ç»„åˆ)
        r1 = random.randint(center - radius, center + radius)
        c1 = random.randint(center - radius, center + radius)
        game.do_move((r1, c1))
        # ç°åœ¨ current_player = 2ï¼Œä»ç¬¬äºŒæ‰‹å¼€å§‹çœŸæ­£è¯„ä¼°

        # ç¡®å®šè°å…ˆæ‰‹ï¼šæ–°æ¨¡å‹åœ¨å¶æ•°å±€å…ˆæ‰‹
        new_starts = (i % 2 == 0)
        move_number = 1

        # ä¸ºä¸¤ä¸ªç©å®¶åˆ›å»º MCTS å®ä¾‹ï¼ˆæ‰©å±•æ—¶å„è‡ªä½¿ç”¨è‡ªå·±çš„æ¨¡å‹ï¼‰
        mcts_new = MCTS(game_class=GameClass, n_simulations=n_simulations, nn_model=model_new, cpuct=cpuct, add_dirichlet_noise=False)
        mcts_best = MCTS(game_class=GameClass, n_simulations=n_simulations, nn_model=model_best, cpuct=cpuct, add_dirichlet_noise=False)

        while not game.is_game_over():
            # æ ¹æ®å½“å‰ç©å®¶å’Œè°å…ˆæ‰‹å†³å®šè°ä¸‹æ£‹
            if (game.current_player == 1 and new_starts) or (game.current_player == 2 and not new_starts):
                pi = mcts_new.run(game, len(game.move_history))
            else:
                pi = mcts_best.run(game, len(game.move_history))

            # ç¡®å®šæ€§é€‰æ‹© (argmax)
            action = int(np.argmax(pi))
            r, c = divmod(action, game.size)
            game.do_move((r, c))
            move_number += 1
            if move_number > game.size * game.size:
                break

        winner = game.get_winner()
        if winner == 0:
            draws += 1
        else:
            # ç¡®å®šæ–°æ¨¡å‹æ˜¯å¦è·èƒœ
            if (winner == 1 and new_starts) or (winner == 2 and not new_starts):
                new_wins += 1

        mcts_new.clear_tree()
        mcts_best.clear_tree()
        del mcts_new
        del mcts_best
        gc.collect()

    win_rate = new_wins / float(total)
    return new_wins, win_rate, draws


# -------------------------
#  å¤šè¿›ç¨‹è¯„ä¼°ï¼ˆå¤–éƒ¨æ¥å£ï¼‰
# -------------------------
def evaluate_models_mp(
    model_new: PyTorchModel,
    model_best: PyTorchModel,
    board_size: int,
    action_size: int,
    n_games: int,
    n_simulations: int,
    cpuct: float,
    *,
    model_dir: str,
    num_workers: int,
    games_per_task: int = 1,
    device: str = "cpu",
    base_seed: int = 54321,
    torch_threads: int = 1,
) -> Tuple[int, float, int]:
    """
    å¹¶è¡Œè¯„ä¼°ï¼šä¿å­˜ä¸¤ä¸ªæ¨¡å‹ checkpoint -> å¤šè¿›ç¨‹å¹¶è¡Œè·‘å¯¹å±€ -> æ±‡æ€»ã€‚
    è¿”å› (new_wins, win_rate, draws)
    """
    os.makedirs(model_dir, exist_ok=True)

    # ä¿å­˜ checkpointï¼ˆé¿å…è·¨è¿›ç¨‹ä¼ é€’æ¨¡å‹å¯¹è±¡ï¼‰
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    ckpt_new = os.path.join(model_dir, f"_eval_model_new_{ts}.pt")
    ckpt_best = os.path.join(model_dir, f"_eval_model_best_{ts}.pt")
    model_new.save(ckpt_new)
    model_best.save(ckpt_best)

    games_per_task = max(1, int(games_per_task))
    tasks = []
    remaining = int(n_games)
    start_idx = 0
    while remaining > 0:
        g = min(games_per_task, remaining)
        tasks.append((start_idx, g))
        start_idx += g
        remaining -= g

    ctx = mp.get_context("spawn")
    total_new_wins = 0
    total_draws = 0
    total_games = 0

    with ProcessPoolExecutor(
        max_workers=int(num_workers),
        mp_context=ctx,
        initializer=_eval_worker_init,
        initargs=(ckpt_new, ckpt_best, board_size, action_size, device, base_seed, torch_threads),
    ) as ex:
        futures = []
        for sidx, gcount in tasks:
            futures.append(
                ex.submit(
                    _eval_play_games,
                    board_size=board_size,
                    n_games=int(gcount),
                    start_index=int(sidx),
                    n_simulations=n_simulations,
                    cpuct=cpuct,
                )
            )

        for fut in as_completed(futures):
            nw, dr, tg = fut.result()
            total_new_wins += int(nw)
            total_draws += int(dr)
            total_games += int(tg)

    # æ¸…ç† checkpoint
    for p in (ckpt_new, ckpt_best):
        try:
            os.remove(p)
        except Exception:
            pass

    win_rate = total_new_wins / float(total_games) if total_games > 0 else 0.0
    return int(total_new_wins), float(win_rate), int(total_draws)


# -------------------------
#  ä¸»è®­ç»ƒå¾ªç¯
# -------------------------
def train_alphazero(
    game_name: str = "gomoku",
    board_size: int = 15,
    num_iterations: int = 5,
    games_per_iteration: int = 8,
    n_simulations: int = 50,
    buffer_size: int = 10000,
    batch_size: int = 128,
    epochs_per_iter: int = 2,
    temp_threshold: int = 8,
    eval_games: int = 12,
    eval_mcts_simulations: int = 200,
    win_rate_threshold: float = 0.55,
    cpuct: float = 1.2,
    model_dir: str = "models",
    save_every: int = 1,
    pretrained_model_path: Optional[str] = None,  # æ–°å‚æ•°ï¼Œç”¨äºä¼ é€’é¢„è®­ç»ƒæ¨¡å‹
    next_iteration_continuation: int = 1,
    # --- MCTS Dirichletå™ªå£°å‚æ•° ---
    dirichlet_alpha: float = 0.03,             # Dirichletå™ªå£°çš„alphaå‚æ•°
    dirichlet_epsilon: float = 0.25,           # Dirichletå™ªå£°çš„æ··åˆæ¯”ä¾‹
    dirichlet_n_moves: int = 30,               # å‰Næ‰‹æ·»åŠ Dirichletå™ªå£°
    # --- å¤šè¿›ç¨‹è‡ªå¯¹å¼ˆå‚æ•° ---
    selfplay_num_workers: int = 0,             # 0=è‡ªåŠ¨ï¼ˆå»ºè®® CPU æ ¸å¿ƒæ•°-1ï¼Œæœ€å¤š8ï¼‰
    selfplay_device: str = "cpu",              # å»ºè®® "cpu"ï¼Œé¿å… CUDA å¤šè¿›ç¨‹äº‰ç”¨
    selfplay_games_per_task: int = 1,          # æ¯ä¸ªä»»åŠ¡åŒ…å«çš„è‡ªå¯¹å¼ˆå±€æ•°ï¼ˆè¶Šå¤§ IPC è¶Šå°‘ï¼‰
    selfplay_base_seed: int = 12345,           # å­è¿›ç¨‹éšæœºç§å­åŸºæ•°
    selfplay_torch_threads: int = 1,           # æ¯ä¸ªå­è¿›ç¨‹å†… torch CPU çº¿ç¨‹æ•°
    # --- å¤šè¿›ç¨‹è¯„ä¼°å‚æ•° ---
    eval_num_workers: int = 0,                 # 0=è‡ªåŠ¨ï¼ˆå»ºè®® CPU æ ¸å¿ƒæ•°-1ï¼Œæœ€å¤š8ï¼‰
    eval_device: str = "cpu",                  # å»ºè®® cpuï¼ˆå¤šè¿›ç¨‹ cuda é£é™©é«˜ï¼‰
    eval_games_per_task: int = 1,              # æ¯ä¸ªä»»åŠ¡è¯„ä¼°å±€æ•°
    eval_base_seed: int = 54321,
    eval_torch_threads: int = 1,
):
    """
    æ ¸å¿ƒè®­ç»ƒæµç¨‹ã€‚
    """
    os.makedirs(model_dir, exist_ok=True)

    # æ ¹æ® board_size è®¡ç®—åŠ¨ä½œç©ºé—´å¤§å°
    action_size = board_size * board_size  # å¯¹äº Gomokuï¼ŒåŠ¨ä½œæ˜¯æ£‹ç›˜ä¸Šçš„ä½ç½®

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹
    if pretrained_model_path and os.path.exists(pretrained_model_path):
        print(f"ä»ä»¥ä¸‹è·¯å¾„åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_model_path}")
        model_best = PyTorchModel(board_size=board_size, action_size=action_size)
        model_best.load(pretrained_model_path)  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model_candidate = PyTorchModel(board_size=board_size, action_size=action_size)
        model_candidate.net.load_state_dict(model_best.net.state_dict())
        print("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    else:
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ã€‚åˆå§‹åŒ–æ–°æ¨¡å‹ã€‚")
        model_best = PyTorchModel(board_size=board_size, action_size=action_size)
        model_candidate = PyTorchModel(board_size=board_size, action_size=action_size)
        # å…³é”®ï¼šè®©å€™é€‰æ¨¡å‹å¤åˆ¶æœ€ä½³æ¨¡å‹çš„åˆå§‹æƒé‡ï¼Œç¡®ä¿ç¬¬ä¸€è½®è¯„ä¼°å…¬å¹³
        model_candidate.net.load_state_dict(model_best.net.state_dict())

    # ç»éªŒå›æ”¾ç¼“å†²åŒº
    buffer_filepath = os.path.join(model_dir, "replay_buffer_latest.pkl")
    
    # å°è¯•åŠ è½½å·²ä¿å­˜çš„ buffer
    buffer = load_replay_buffer(buffer_filepath, capacity=buffer_size)
    
    # å¦‚æœåŠ è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ç©º buffer
    if buffer is None:
        print("[Buffer] åˆ›å»ºæ–°çš„ç©º buffer")
        buffer = ReplayBuffer(capacity=buffer_size)
    else:
        print(f"[Buffer] æˆåŠŸåŠ è½½å†å² bufferï¼Œå½“å‰å¤§å°: {len(buffer)}/{buffer_size}")

    # æ¸©åº¦è°ƒåº¦
    def temp_fn(move_number: int):
        return max(0.0, 1.0 - move_number / temp_threshold)

    for it in range(next_iteration_continuation, next_iteration_continuation + num_iterations):
        t0 = time.time()
        print(f"\n=== ITER {it}/{next_iteration_continuation + num_iterations - 1}: è‡ªå¯¹å¼ˆç”Ÿæˆ (games={games_per_iteration}, sims={n_simulations}), å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
        selfplay_t0 = time.time()

        # ä½¿ç”¨å€™é€‰æ¨¡å‹è¿›è¡Œè‡ªå¯¹å¼ˆç”Ÿæˆï¼ˆå¤šè¿›ç¨‹ï¼‰
        winners = {0: 0, 1: 0, 2: 0}

        # è‡ªåŠ¨ worker æ•°ï¼šCPU æ ¸å¿ƒæ•°-1ï¼Œæœ€å¤š 8ï¼Œæœ€å°‘ 1
        if selfplay_num_workers and selfplay_num_workers > 0:
            num_workers = int(selfplay_num_workers)
        else:
            cpu_cnt = os.cpu_count() or 2
            num_workers = max(1, min(8, cpu_cnt - 1))

        # 1 worker ç­‰ä»·ä¸²è¡Œï¼ˆä½†ä»èµ°åŒä¸€å¥—ä»£ç è·¯å¾„ï¼‰
        max_moves = board_size * board_size
        use_symmetries = True
        add_dirichlet_noise = True

        # å¹¶è¡Œæ‰§è¡Œ
        if num_workers == 1:
            # ä¸²è¡Œï¼šç›´æ¥ä½¿ç”¨ä¸»è¿›ç¨‹çš„ model_candidateï¼ˆé¿å…å¤šä½™çš„ save/loadï¼‰
            for g in range(games_per_iteration):
                mcts_play = MCTS(
                    game_class=GameClass,
                    n_simulations=n_simulations,
                    nn_model=model_candidate,
                    cpuct=cpuct,
                    dirichlet_alpha=dirichlet_alpha,
                    epsilon=dirichlet_epsilon,
                    apply_dirichlet_n_first_moves=dirichlet_n_moves,
                    add_dirichlet_noise=add_dirichlet_noise,
                )
                game = GameClass(size=board_size)
                game.current_player = 1
                examples, winner = play_game_and_collect(
                    mcts_play, game, temp_fn, max_moves=max_moves, use_symmetries=use_symmetries
                )
                buffer.add(examples)
                winners[winner] = winners.get(winner, 0) + 1

                mcts_play.clear_tree()
                del mcts_play
                gc.collect()
        else:
            # ä¿å­˜å€™é€‰æ¨¡å‹åˆ° checkpointï¼Œå­è¿›ç¨‹ä»ç£ç›˜åŠ è½½ï¼ˆé¿å…ä¼ é€’ä¸å¯ pickle çš„æ¨¡å‹å¯¹è±¡ï¼‰
            selfplay_ckpt_path = os.path.join(model_dir, f"_selfplay_candidate_iter{it}.pt")
            model_candidate.save(selfplay_ckpt_path)

            # ä»»åŠ¡åˆ‡åˆ†
            games_per_task = max(1, int(selfplay_games_per_task))
            tasks = []
            remaining = int(games_per_iteration)
            while remaining > 0:
                g = min(games_per_task, remaining)
                tasks.append(g)
                remaining -= g

            gen_games_done = 0
            ctx = mp.get_context("spawn")
            with ProcessPoolExecutor(
                max_workers=num_workers,
                mp_context=ctx,
                initializer=_selfplay_worker_init,
                initargs=(selfplay_ckpt_path, board_size, action_size, selfplay_device, selfplay_base_seed, selfplay_torch_threads),
            ) as ex:
                futures = []
                for gcount in tasks:
                    futures.append(
                        ex.submit(
                            _selfplay_generate_games,
                            board_size=board_size,
                            n_simulations=n_simulations,
                            cpuct=cpuct,
                            temp_threshold=temp_threshold,
                            add_dirichlet_noise=add_dirichlet_noise,
                            games_to_play=int(gcount),
                            use_symmetries=use_symmetries,
                            max_moves=max_moves,
                            dirichlet_alpha=dirichlet_alpha,
                            dirichlet_epsilon=dirichlet_epsilon,
                            dirichlet_n_moves=dirichlet_n_moves,
                            device=selfplay_device,
                        )
                    )

                for fut in as_completed(futures):
                    examples, w = fut.result()
                    buffer.add(examples)
                    for k, v in w.items():
                        winners[k] = winners.get(k, 0) + int(v)
                    gen_games_done += int(sum(w.values()))

            # å¯é€‰ï¼šæ¸…ç† selfplay checkpointï¼ˆä¿ç•™ä¹Ÿå¯ä»¥æ–¹ä¾¿å¤ç°å®éªŒï¼‰
            try:
                os.remove(selfplay_ckpt_path)
            except Exception:
                pass

        selfplay_t1 = time.time()
        print(f"è‡ªå¯¹å¼ˆå®Œæˆï¼šè€—æ—¶ {(selfplay_t1 - selfplay_t0)/60:.2f}åˆ†é’Ÿï¼Œèƒœè´Ÿç»Ÿè®¡={winners}ï¼Œbuffer_size={len(buffer)}")

        # å¦‚æœæœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œè®­ç»ƒå€™é€‰æ¨¡å‹
        if len(buffer) >= batch_size:
            print(f"\nTraining candidate model: buffer={len(buffer)}, batch_size={batch_size}, epochs_per_iter={epochs_per_iter}")
            n_batches = max(1, len(buffer) // batch_size)
            for epoch in range(epochs_per_iter):
                epoch_t0 = time.time()
                for b in range(n_batches):
                    states_b, pis_b, zs_b = buffer.sample(batch_size)
                    loss_info = model_candidate.train_batch(states_b, pis_b, zs_b, epochs=1)
                epoch_t1 = time.time()
                print(f"  epoch {epoch+1}/{epochs_per_iter} finished in {epoch_t1 - epoch_t0:.1f}s, last_loss={loss_info}")
        else:
            print(f"è®­ç»ƒæ ·æœ¬ä¸è¶³ (buffer={len(buffer)}, éœ€è¦ {batch_size})ã€‚è·³è¿‡æœ¬æ¬¡è¿­ä»£çš„è®­ç»ƒã€‚")

        # è¯„ä¼°ï¼ˆç²¾ç®€è¾“å‡ºï¼šåªåœ¨ç»“æŸåæ±‡æ€»ä¸€æ¬¡ï¼‰
        eval_t0 = time.time()
        try:
            # è‡ªåŠ¨è¯„ä¼°è¿›ç¨‹æ•°ï¼ˆä¸è‡ªå¯¹å¼ˆåŒç­–ç•¥ï¼‰
            if eval_num_workers and eval_num_workers > 0:
                eval_workers = int(eval_num_workers)
            else:
                cpu_cnt = os.cpu_count() or 2
                eval_workers = max(1, min(8, cpu_cnt - 1))

            if eval_workers == 1:
                new_wins, win_rate, draws = evaluate_models(
                    model_candidate,
                    model_best,
                    game_name,
                    n_games=eval_games,
                    n_simulations=eval_mcts_simulations,
                    cpuct=cpuct,
                )
            else:
                new_wins, win_rate, draws = evaluate_models_mp(
                    model_candidate,
                    model_best,
                    board_size=board_size,
                    action_size=action_size,
                    n_games=eval_games,
                    n_simulations=eval_mcts_simulations,
                    cpuct=cpuct,
                    model_dir=model_dir,
                    num_workers=eval_workers,
                    games_per_task=eval_games_per_task,
                    device=eval_device,
                    base_seed=eval_base_seed,
                    torch_threads=eval_torch_threads,
                )
        except Exception as e:
            # ä¿æŒå¯è§æ€§ï¼Œä½†ä¸åˆ·å±
            print(f"è¯„ä¼°å¤±è´¥ï¼š{e}")
            new_wins, win_rate, draws = 0, 0.0, 0

        eval_t1 = time.time()
        print(
            f"è¯„ä¼°å®Œæˆï¼šè€—æ—¶ {(eval_t1 - eval_t0)/60:.2f} åˆ†é’Ÿï¼Œèƒœç‡={win_rate:.3f}ï¼ˆ{new_wins}/{eval_games}ï¼‰ï¼Œå¹³å±€={draws}"
        )

        # æ¥å—/æ‹’ç»
        if win_rate >= win_rate_threshold:
            print(" å€™é€‰æ¨¡å‹è¢«æ¥å— -> æå‡ä¸ºæœ€ä½³æ¨¡å‹ã€‚")
            # æ›´æ–° model_bestï¼ˆæ·±æ‹·è´æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
            model_best.net.load_state_dict(model_candidate.net.state_dict())
            model_best.optimizer.load_state_dict(model_candidate.optimizer.state_dict())
            # ä»æœ€ä½³æ¨¡å‹åˆ›å»ºæ–°çš„å€™é€‰æ¨¡å‹
            model_candidate = PyTorchModel(board_size=board_size, action_size=action_size)
            model_candidate.net.load_state_dict(model_best.net.state_dict())
            model_candidate.optimizer.load_state_dict(model_best.optimizer.state_dict())
        else:
            print(" å€™é€‰æ¨¡å‹è¢«æ‹’ç» -> ä»æœ€ä½³æ¨¡å‹æ¢å¤å€™é€‰æ¨¡å‹ï¼ˆé‡ç½®ä¼˜åŒ–å™¨ï¼‰ã€‚")
            # ä»æœ€ä½³æ¨¡å‹æƒé‡é‡ç½®å€™é€‰æ¨¡å‹ï¼Œä½†ä¸ç»§æ‰¿ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆç»™äºˆæ–°çš„å¼€å§‹ï¼‰
            model_candidate = PyTorchModel(board_size=board_size, action_size=action_size)
            model_candidate.net.load_state_dict(model_best.net.state_dict())

        # å®šæœŸä¿å­˜æ¨¡å‹å¿«ç…§
        if it % save_every == 0:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            snapshot_path = os.path.join(model_dir, f"snapshot_iter{it}_{timestamp}.pt")
            model_best.save(snapshot_path)
            print(f" ğŸ’¾ Saved snapshot: {snapshot_path}")
        
        # æ¯è½®éƒ½ä¿å­˜ bufferï¼ˆè¦†ç›–æ—§çš„ï¼Œåªä¿ç•™æœ€æ–°ï¼‰
        save_replay_buffer(buffer, buffer_filepath)

        t1 = time.time()
        print(f"è¿­ä»£ {it} å®Œæˆï¼Œè€—æ—¶ {(t1 - t0)/60:.2f}åˆ†é’Ÿã€‚æœ¬æ¬¡è¿­ä»£è·èƒœè€…: {winners}")

    print("\n=== è®­ç»ƒå®Œæˆ ===")

# -------------------------
#  å…¥å£ç‚¹
# -------------------------
if __name__ == "__main__":
    mp.freeze_support()
    train_alphazero(
        game_name="gomoku",           # æ¸¸æˆ Gomoku
        board_size=15,                # æ£‹ç›˜å¤§å° (15x15)

        num_iterations=300,           # 30 æ¬¡è®­ç»ƒè¿­ä»£
        games_per_iteration=70,       # æ¯æ¬¡è¿­ä»£ 70 å±€æ¸¸æˆ

        n_simulations=1600,          # MCTS 1600 æ¬¡æ¨¡æ‹Ÿ
        cpuct=1.0,                   # MCTS çš„æ¢ç´¢/åˆ©ç”¨å¹³è¡¡å› å­

        buffer_size=60000,           # ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œæœ€å¤š 60,000 ä¸ªæ ·æœ¬
        batch_size=128,               # æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ 128 ä¸ªæ ·æœ¬
        epochs_per_iter=5,           # æ¯æ¬¡è¿­ä»£ 3 ä¸ªè®­ç»ƒè½®æ¬¡

        temp_threshold=10,           # æ¢ç´¢æ¸©åº¦é˜ˆå€¼
        eval_games=60,               # 50 å±€è¯„ä¼°æ¸¸æˆï¼ˆæé«˜ç»Ÿè®¡ç¨³å®šæ€§ï¼‰
        eval_mcts_simulations=1600,  # è¯„ä¼°æ—¶ MCTS 1600 æ¬¡æ¨¡æ‹Ÿ
        win_rate_threshold=0.5,     # å¦‚æœå€™é€‰æ¨¡å‹èƒœç‡è¾¾åˆ° 52% åˆ™æ¥å—

        # Dirichletå™ªå£°å‚æ•°ï¼ˆAlphaZeroæ ‡å‡†é…ç½®ï¼‰
        dirichlet_alpha=0.05,        # Dirichletå™ªå£°çš„alphaå‚æ•°ï¼ˆå›´æ£‹è®ºæ–‡æ ‡å‡†å€¼ï¼‰
        dirichlet_epsilon=0.15,      # å™ªå£°æ··åˆæ¯”ä¾‹ï¼ˆæ ¹èŠ‚ç‚¹æ¢ç´¢ï¼‰
        dirichlet_n_moves=10,        # å‰30æ‰‹æ·»åŠ å™ªå£°ï¼ˆå¢åŠ å¼€å±€å¤šæ ·æ€§ï¼‰

        model_dir="models",          # ä¿å­˜æ¨¡å‹çš„ç›®å½•
        save_every=1,                # æ¯æ¬¡è¿­ä»£ä¿å­˜æ¨¡å‹
        pretrained_model_path="models/snapshot_iter140_20260109_190822.pt",  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆNone è¡¨ç¤ºä»å¤´è®­ç»ƒï¼‰

        next_iteration_continuation=141,  # ä»ç¬¬ 101 æ¬¡è¿­ä»£å¼€å§‹

        # å¤šè¿›ç¨‹è‡ªå¯¹å¼ˆï¼š28 ä¸ªè¿›ç¨‹
        selfplay_num_workers=28,
        selfplay_device="cpu",
        selfplay_games_per_task=1,
        selfplay_torch_threads=1,

        # å¤šè¿›ç¨‹è¯„ä¼°ï¼š28 ä¸ªè¿›ç¨‹
        eval_num_workers=28,
        eval_device="cpu",
        eval_games_per_task=1,
        eval_torch_threads=1,
    )